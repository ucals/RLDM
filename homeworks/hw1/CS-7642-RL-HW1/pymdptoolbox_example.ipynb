{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pymdptoolbox Introduction Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to take a MDP graph and represent it using pymdptoolbox in python.  Then we will use Value Iteration to find the optimal policy and expected value of the given mdp\n",
    "\n",
    "## The problem\n",
    "A forest is managed by two actions: ‘Wait’ and ‘Cut’. An action is decided each year with first the objective to maintain an old forest for wildlife and second to make money selling cut wood Each year. There is a probability p that a fire burns the forest.\n",
    "As showed in visual representation,blue line is probalibity, transitions.For each year there is 30% chance a fire will burns the forest and bring the state to 0(1 in graph). In comparision, there is 70% change the forest will endure and get into the next state. \n",
    "As showed in visual representation,black line is reward. For the first your years, there is no money reward for waiting, plus suffering fire burns.If cut the tree there will be some benefits (reward=1). But once the tree reach the fifth year (state 4), it will start to maintain wildlife (reward =0.3) and have doubled money reward (reward =1)once cut.\n",
    "### Visual Representation \n",
    "![alt text](./mdp.jpeg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "The first thing we need to do is setup matricies for the transition probablities and the rewards.  \n",
    "\n",
    "The transition probablities will be represented in a num actions x num states x num states matrix\n",
    "\n",
    "The rewards will be represented in a num states x num actions array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "prob = np.zeros((2, 5, 5)) #creat 2 empty 5 X 5 array\n",
    "\n",
    "prob[0] = [[0.3, 0.7, 0., 0., 0.],\n",
    "           [0.3, 0.0, 0.7, 0., 0.],\n",
    "           [0.3, 0.0, 0., 0.7, 0.],\n",
    "           [0.3, 0.0, 0., 0., 0.7],\n",
    "           [0.3, 0.0, 0., 0., 0.7]]\n",
    "\n",
    "prob[1] = [[1., 0., 0., 0., 0.],\n",
    "           [1., 0., 0., 0., 0.],\n",
    "           [1., 0., 0., 0., 0.],\n",
    "           [1., 0., 0., 0., 0.],\n",
    "           [1., 0., 0., 0., 0.]]\n",
    "\n",
    "rewards = np.zeros((5, 2))\n",
    "rewards[0] = [0., 0.]\n",
    "rewards[1] = [0., 1.]\n",
    "rewards[2] = [0., 1.]\n",
    "rewards[3] = [0., 1.]\n",
    "rewards[4] = [0.3, 2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Now we need to setup the MDP in pymdptoolbox and run Value Iteration to get the expected value and optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "vi = mdptoolbox.mdp.ValueIteration(prob, rewards, 0.9)\n",
    "vi.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can extract the optimal policy and expected value of each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_policy = vi.policy\n",
    "expected_values = vi.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Putting it all together\n",
    "\n",
    "Here is the final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import numpy as np\n",
    "\n",
    "prob = np.zeros((2, 5, 5))\n",
    "\n",
    "prob[0] = [[0.3, 0.7, 0., 0., 0.],\n",
    "           [0.3, 0.0, 0.7, 0., 0.],\n",
    "           [0.3, 0.0, 0., 0.7, 0.],\n",
    "           [0.3, 0.0, 0., 0., 0.7],\n",
    "           [0.3, 0.0, 0., 0., 0.7]]\n",
    "\n",
    "prob[1] = [[1., 0., 0., 0., 0.],\n",
    "           [1., 0., 0., 0., 0.],\n",
    "           [1., 0., 0., 0., 0.],\n",
    "           [1., 0., 0., 0., 0.],\n",
    "           [1., 0., 0., 0., 0.]]\n",
    "\n",
    "rewards = np.zeros((5, 2))\n",
    "rewards[0] = [0., 0.]\n",
    "rewards[1] = [0., 1.]\n",
    "rewards[2] = [0., 1.]\n",
    "rewards[3] = [0., 1.]\n",
    "rewards[4] = [0.3, 2.]\n",
    "\n",
    "vi = mdptoolbox.mdp.ValueIteration(prob, rewards, 0.9)\n",
    "vi.run()\n",
    "\n",
    "optimal_policy = vi.policy\n",
    "expected_values = vi.V\n",
    "\n",
    "print(optimal_policy)\n",
    "print(expected_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
