@article{dqn,
    added-at = {2015-08-26T14:46:40.000+0200},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
    biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
    description = {Human-level control through deep reinforcement learning - nature14236.pdf},
    interhash = {eac59980357d99db87b341b61ef6645f},
    intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
    issn = {00280836},
    journal = {Nature},
    keywords = {deep learning toread},
    month = feb,
    number = 7540,
    pages = {529--533},
    publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
    timestamp = {2015-08-26T14:46:40.000+0200},
    title = {Human-level control through deep reinforcement learning},
    url = {http://dx.doi.org/10.1038/nature14236},
    volume = 518,
    year = 2015
}

@inproceedings{ddqn,
    author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
    title = {Deep Reinforcement Learning with Double Q-Learning},
    booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
    series = {AAAI'16},
    year = {2016},
    location = {Phoenix, Arizona},
    pages = {2094--2100},
    numpages = {7},
    url = {http://dl.acm.org/citation.cfm?id=3016100.3016191},
    acmid = {3016191},
    publisher = {AAAI Press},
}

@InProceedings{dueling,
    title = 	 {Dueling Network Architectures for Deep Reinforcement Learning},
    author = 	 {Ziyu Wang and Tom Schaul and Matteo Hessel and Hado Hasselt and Marc Lanctot and Nando Freitas},
    booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
    pages = 	 {1995--2003},
    year = 	 {2016},
    editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
    volume = 	 {48},
    series = 	 {Proceedings of Machine Learning Research},
    address = 	 {New York, New York, USA},
    month = 	 {20--22 Jun},
    publisher = 	 {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v48/wangf16.pdf},
    url = 	 {http://proceedings.mlr.press/v48/wangf16.html},
    abstract = 	 {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.}
}

@article{per,
    title={Prioritized Experience Replay},
    author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
    journal={CoRR},
    year={2016},
    volume={abs/1511.05952}
}

@misc{openai,
    abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a
growing collection of benchmark problems that expose a common interface, and a
website where people can share their results and compare the performance of
algorithms. This whitepaper discusses the components of OpenAI Gym and the
design decisions that went into the software.},
    added-at = {2018-04-12T12:08:39.000+0200},
    author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
    biburl = {https://www.bibsonomy.org/bibtex/2cdc8f927d6c8657ea82951a09e34161a/achakraborty},
    description = {[1606.01540] OpenAI Gym},
    interhash = {cfd0ba0b44eda9a3ca67480dfbf823a0},
    intrahash = {cdc8f927d6c8657ea82951a09e34161a},
    keywords = {2016 arxiv paper reinforcement-learning},
    note = {cite arxiv:1606.01540},
    timestamp = {2018-04-12T12:08:39.000+0200},
    title = {OpenAI Gym},
    url = {http://arxiv.org/abs/1606.01540},
    year = 2016
}

@phdthesis{qlearning,
    added-at = {2008-03-11T14:52:34.000+0100},
    author = {Watkins, C. J. C. H.},
    biburl = {https://www.bibsonomy.org/bibtex/21ffd549077ea1da7675431a17fa2af03/idsia},
    citeulike-article-id = {2381652},
    interhash = {ca824d64b71939208358edb4a26f8351},
    intrahash = {1ffd549077ea1da7675431a17fa2af03},
    keywords = {juergen},
    priority = {2},
    school = {King's College, Oxford},
    timestamp = {2008-03-11T14:53:51.000+0100},
    title = {Learning from Delayed Rewards},
    year = 1989
}








