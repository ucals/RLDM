\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{textgreek}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algpascal}
\usepackage{graphicx}
\usepackage[
backend=biber,
style=numeric,
sorting=ynt,
citestyle=authoryear
]{biblatex}
\addbibresource{paper.bib}

%\graphicspath{ {./images/} }

\title{Playing Lunar Lander with Deep Reinforcement Learning}

\date{June 28th, 2019}

\author{
Carlos Souza\thanks{Latest Git hash: INCLUDE LATEST GITHASH}\\
%Department of Computer Science\\
Georgia Institute of Technology\\
SÃ£o Paulo, SP, Brazil \\
\texttt{souza@gatech.edu} \\
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\begin{document}
    \maketitle

    \begin{abstract}
        \lipsum[1]
    \end{abstract}


    % keywords can be removed
    \keywords{Reinforcement learning \and Deep Reinforcement Learning \and Deep Q-Networks}
    %\githash{DO NOT FORGET TO INCLUDE LATEST GIT HASH}


    \section{Introduction}
    \label{sec:introduction}
    Reinforcement learning (RL) theory provides mathematical background and classic algorithms to enable agents to learn optimally control an environment.
    However, to successfully apply those algorithms in problems with real-world complexity, several challenges must be tackled, must notably i) how to derive efficient representations of the environment from high-dimensional sensory inputs, and ii) how to use these to generalize past experience to new situation (\cite{dqn}).

    Recent advances in deep neural networks (DNNs) originated a new type of agent, known as Deep Q-Network agent, capable of overcoming these challenges, which enabled RL algorithms to perform effectively.
    All the many recent successes in applying RL to complex sequential decision-making problems were kick-started by the Deep Q-Network algorithm (DQN; \cite{dqn}).
    Since then, many extensions have been proposed to improve its stability and/or speed.
Double DQN (DDQN; \cite{ddqn}) addresses the problem of overestimation of action values, by decomposing the max operation in the target into action selection and action evaluation.
    The Dueling Network architecture (Dueling DDQN; \cite{dueling}) better generalize learning accross actions by proposing an architecture that explicitly separates the representation of state values and (state-dependent) action advantages.
    Prioritized Experience Replay (PER; \cite{per}) improves data efficiency, by replaying more often transitions from which there is more to learn.

    Each of these algorithms enables substantial performance improvements in isolation and combined, since they build on a shared framework.
    In this paper we propose to study an agent that combines all the aforementioned improvements, exploring its performance in Lunar Lander environment from OpenAI gym (\cite{openai}).

    \subsection{Background}
    \label{subsec:background}
    In the standard reinforcement learning setting, agent interacts with an environment over discrete time steps.
    At each time step $t$, the agent perceives a state $s_{t} \in \mathcal{S}$, chooses an action from a discrete set $a_{t} \in \mathcal{A}$ accordingly to a policy $\pi$, where $\pi$ is a mapping from state space $\mathcal{S}$ to action space $\mathcal{A}$, and observes a reward $r_{t}$ and a next state $s_{t+1}$.
    This process continues until the agent reaches a terminal state, after which the process is restarted.
    The purpose of the agent is to maximize the expected discounted reward $R_{t} = \sum_{k=1}^\infty \gamma^k r_{k+t}$, where $\gamma \in (0, 1]$ is the discount factor that trades-off the importance of immediate vs future rewards.
    This interaction between the agent and the environment is formalized as a \emph{Markov Decision Process} (MDP), described by $\langle \mathcal{S}, \mathcal{A}, T, r, \gamma \rangle$ tuple, where $T(s, a, s') = P[s_{t+1} = s' | s_{t} = s, a_{t} = a]$ is the stochastic transition function.

    For an agent following a stochastic policy $\pi$, the true value of an action $a$ in a state $s$, and the value of that state $s$ are:
    \begin{align}
        Q^{\pi}(s, a) &= \mathbb{E}[R_{t} | s_{t} = s, a_{t} = a], \text{and} \label{eq:1} \\
        V^{\pi}(s) &= \mathbb{E}_{a\sim \pi(s)}[Q^{\pi}(s, a)] \label{eq:2}
    \end{align}
    The optimal state-value function is defined as $Q^{*}(s, a) = \max_{\pi}Q^{\pi}(s, a)$.
    Under deterministic policy $a = \argmax_{a' \in \mathcal{A}}Q^{*}(s, a')$, it follows that $V^{*} = \max_{a}Q^{*}(s, a)$.
    Also, the optimal state-value function $Q$ satisfies the Bellman equation:
    \begin{equation}
        Q^{*}(s, a) = \mathbb{E}_{s'}\left[r + \gamma \max_{a'} Q^{*}(s', a') \:|\: s, a\right]
    \end{equation}

    A common way of deriving a new policy from a state-action value function is to act $\epsilon$-greedly with respect to the action values, i.e. taking the action with the highest value with probability $(1 - \epsilon)$ or otherwise acting randomly with probability $\epsilon$.
    The optimal policy is easily derived from the optimal state-action value function by selecting the highest-valued action in each state.

    One of the early breakthroughs in RL was the development of an off-policy Temporal Difference control algorithm known as \emph{Q-learning} (\cite{qlearning}), defined by:
    \begin{equation}
        Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha \left[ r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_{t}, a_{t}) \right],
    \end{equation}
    where $\alpha$ is a constant step-size parameter, or learning rate.
    The learned action-value function $Q$ directly approximates the \emph{optimal} action-value function $Q^{*}$, independent of the policy being followed.

    However, large state and/or action spaces make it tractable to learn $Q$ value estimates for each state-action pairs independently.
    To solve this challenge, DQN (\cite{dqn}) successfully used deep neural networks to approximate the state-action value function as $Q(s, a; \theta)$, where $\theta$ are the parameters of the network.
    For an $n$-dimensional state space and an action space containing $m$ actions, the neural network is a function from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$.

    Two important components of the DQN algorithm proposed by \cite{dqn} are i) the use of a target network, and ii) the use of experience replay.
    At each time step, the agent selects an action $\epsilon$-greedily with respect to the action values, and adds a transition $\langle s_{t}, a_{t}, r_{t}, s_{t+1} \rangle$ to the experience replay memory, that holds millions of experience tuples.
    The parameters of the neural network are then optimized by using stochastic gradient descent to minimize the loss function at iteration $i$:
    \begin{align}
        L_{i}(\theta_{i}) &= \mathbb{E}_{s, a, r, s'} \left[ \left( y_{i} - Q(s, a; \theta_{i}) \right)^2 \right] \\
        y_{i} &= r + \gamma \max_{a'}Q(s', a'; \theta^{-})
    \end{align}
    where $\theta^{-}$ are the parameters of a fixed and separate \emph{target network}.
    The parameters of the target network $Q(s', a'; \theta^{-})$ are frozen for a fixed number of iterations while updating the \emph{online network} $Q(s, a; \theta_{i})$ by gradient descent.
    The optimization is performed on mini-batches sampled uniformly at random from the experience replay memory.
    These two additions greatly improves the stability of the algorithm, leading to super-human performance on several Atari games.

    \subsection{DQN Extensions}
    \label{subsec:extensions}

    \paragraph{Double Q-learning.}
    The max operator in standard Q-learning and DQN uses the same values both to select and to evaluate an action.
    This makes it more likely to select overestimated values, resulting in overoptimistic value estimates.
    To prevent this, we can decouple the selection from the evaluation.
    This is the idea behind Double Q-learning.
    It is possible to effectively combine this with DQN using the loss
    \begin{align}
        L_{i}(\theta_{i}) &= \mathbb{E}_{s, a, r, s'} \left[ \left( y_{i} - Q(s, a; \theta_{i}) \right)^2 \right] \\
        y_{i} &= r + \gamma Q(s', \argmax_{a'} Q(s', a'; \theta_{i}); \theta^{-}),
    \end{align}
    which was shown to improve performance by reducing value overestimates present in vanilla DQN.

    \paragraph{Dueling networks.}
    The dueling network is a DNN architecture designed for value based RL.
    The key insight behind it is that, for many states, it is unnecessary to estimate the value of each action choice.
    After the first DQN original layers, two streams of fully-connected layers are constructed such that they have the capability of providing separate estimates of the value and advantage functions, being the \emph{advantage function} defined as $A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)$.
    Finally, the two streams are combined to produce a single output $Q$ function
    \begin{equation}
        Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \left( A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a'; \theta, \alpha) \right),
    \end{equation}
    where $\theta$ denotes the parameters of the previous layers, while $\alpha$ and $\beta$ are the parameters of the two streams of fully-connected layers.
    As the dueling architecture shares the same input-output interface with standard Q networks, we can recycle all learning algorithms with Q networks to train the dueling architecture.

    \paragraph{Prioritized experience replay.}
    DQN samples uniformly from the replay memory.
    Their key idea behind prioritized experience replay is to increase the replay probability of experience tuples from which there is much to learn.
    \cite{per} proposes the last encountered absolute \emph{TD error} as a proxy for learning potential, which is the central component of the algorithm.

    Several implementation challenges arise from the greedy TD-error prioritization and the bias it introduces by changing the distribution of experience samples.
    To overcome those, the algorithm implements \emph{stochastic prioritization} and \emph{weighted importance-sampling}.

    Prioritized experience replay is shown to deliver both faster learning and better final policy quality across most games of the Atari benchmark suite, as compared to uniform experience replay.

    \subsection{Lunar Lander Environment}
    \label{subsec:lunarlander}
    Rocket trajectory optimization is a classic topic in Optimal Control.
    OpenAI Gym proposes an environment where an agent has to control a "Lunar Lander" with the purpose of landing it successfully in a landing pad.
    The agent has four possible actions: do nothing, fire the left orientation engine, fire the main engine, or fire the right orientation engine.
    The state space is a 8-dimensional continuous space: at each time step, environment returns Lunar Lander position (x, y), velocities (horizontal and vertical), angle, angular velocity, and whether left/right legs are in contact with the ground.
    Landing pad coordinates are (0,0).
    Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.
    If lander moves away from landing pad it loses reward back.
    Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points.
    Each leg ground contact is +10.
    Firing main engine is -0.3 points each frame.
    Firing side engine is -0.03 points each frame.
    The problem is considered to be solved when the agent achieves 200 score over past 100-episodes rolling average.

    \section{Methods}
    \label{sec:methods}
    To investigate how the integrated agent that combines all described improvements perform, using Lunar Lander environment as test-bed, we propose 5 different experiments detailed below.

    \paragraph{Experiment 1: Impact of DQN extensions.}
    We start our study by analyzing the impact of each DQN extension, comparing all possible combinations.
    The simplest agent implements vanilla DQN, while the most sophisticated, the integrated agent, combines all extensions described earlier.
    We also compared these with agents containing only one and/or two extensions, to understand the impact of each extension on their performance.

    While varying the extensions, we kept all other hyper-parameters frozen.
    We use RMSProp as the optimization algorithm, with $\alpha$ (learning rate) of $0.0001$.
    $\gamma$ (discount factor) is set at $0.99$.
    Replay memory capacity is set to $10,000$ experience tuples, and training is performed using $32$ as mini-batch size.
    The network architecture consists of 2 fully-connected hidden layers with 512 neurons each, both with rectified linear unit (ReLU) activation functions (for dueling network: we added a fully-connected layer as described previously).
    For prioritized replay, we annealed the bias by progressively increasing $\beta$ (exponent in importance-sampling weights) from $0.4$ to $1$.
    As exploration strategy, we decayed $\epsilon$ following $\epsilon(t) = e^{-t/70}$, where $t$ is the current episode.
    If $\epsilon$ falls below $0.05$, it stays constant at $0.05$ from that point on.

    \paragraph{Experiment 2: Effect of different gradient descent optimization algorithms and learning rates.}
    Next, we assess the effect of different optimization algorithms and their optimal learning rates on the integrated agent, which combines all DQN extensions.
    We simulate RMSProp, Adaptive Moment Estimation (Adam), and Stochastic Gradient Descent (SGD).
    For those 3 different algorithms, we test $30$ different $\alpha$ (learning rates), evenly spaced on a log scale from $10^{-6}$ to $10^{-1}$.
    All other hyper-parameters are kept frozen as described in experiment 1.

    \paragraph{Experiment 3: Discount factor sensitivity.}
    In this experiment, we investigate the effect of $\gamma$ in the integrated agent's performance.
    We test all $\gamma \in [0.8, 0.85, 0.9, 0.92, 0.94, 0.96, 0.98, 0.99, 1.0]$.
    As usual, all other hyper-parameters are kept frozen.

    \paragraph{Experiment 4: Evaluation of different exploration strategies.}
    Following, we evaluate the impact of $10$ different $\epsilon$ decay curves.
    We assess two types of curves: exponential decay functions, $\epsilon(t) = e^{-k t}$; and inverted logistic functions, $\epsilon(t) = 1 / (1 + e^{-k_{1} (t - k_{2})})$.
    Again, all other hyper-parameters are kept frozen.

    \paragraph{Experiment 5: Effect of kick-starting replay memory with human experience.}
    Finally, we investigate how kick-starting the replay memory with human experience can speed-up agent learning.
    To do it, we generate experience tuples $\langle s, a, s', r \rangle$ by playing Lunar Lander by hand, and use this data to pre-populate the replay memory.
    Then, we set our integrated agent to start learning procedure, with $\epsilon$ constant at $0.05$.

    \section{Results}
    \label{sec:results}

    \lipsum[1]
    \lipsum[2]

    \lipsum[3]
    \lipsum[4]

    \lipsum[5]
    \lipsum[6]

    \lipsum[1]
    \lipsum[2]

    \lipsum[3]
    \lipsum[4]

    \lipsum[5]
    \lipsum[6]

    \section{Conclusion}
    \label{sec:conclusion}

    \lipsum[1]

\printbibliography

\end{document}
