{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pymdptoolbox Introduction Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to take a MDP graph and represent it using pymdptoolbox in python.  Then we will use Value Iteration to find the optimal policy and expected value of the given mdp\n",
    "\n",
    "## The problem\n",
    "The game DieN is played in the following way.\n",
    "Consider a die with N sides (where N is an integer greater than 1) and a nonempty set B of integers. The rules of the game are:\n",
    "1. You start with 0 dollars.\n",
    "2. Roll an N-sided die with a different number from 1 to N printed on each side.\n",
    "a. If you roll a number not in B, you receive that many dollars. (eg. if you roll the number 2 and 2 is not in B, then you receive 2 dollars.)\n",
    "b. If you roll a number in B, then you lose all of your obtained money and the game ends.\n",
    "3. After you roll the die (and don’t roll a number in B), you have the option to quit the game.If you quit, you keep all the money you’ve earned to that point. If you continue to roll, go back to step 2.\n",
    "\n",
    "\n",
    "### Visual Representation \n",
    "![alt text](./mdp.jpeg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "##The first thing we need to do is setup matricies for the transition probablities and the rewards.  \n",
    "Number of states is equal to total possible bankroll in the game. for N = 6, isBadSide = {1,1,1,0,0,0}, possible bankroll after roll 1 are {0,4,5,6}; after roll 2 are {0,8,9,10,11,12}.Possible bankroll states increase as fibonacci series list.But I will also include all non-exsit bankroll just for convience.\n",
    "I will use truncated matrix no more than roll 2 for DieN = 6.\n",
    "There is one more state call quit. quit != leave\n",
    "You can choice to leave(action=0), but you are force to quit (s in B)\n",
    "state {quit\t0\t4\t5\t6\t8\t9\t10\t11\t12}\n",
    "\n",
    "The transition probablities will be represented in a num actions x num states x num states matrix\n",
    "\n",
    "The rewards will be represented in a num states x num actions array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Now we need to setup the MDP in pymdptoolbox and run Value Iteration to get the expected value and optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mdptoolbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b73fd74421f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmdptoolbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdptoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValueIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mdptoolbox'"
     ]
    }
   ],
   "source": [
    "import mdptoolbox\n",
    "vi = mdptoolbox.mdp.ValueIteration(prob, rewards, 1)\n",
    "vi.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can extract the optimal policy and expected value of each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_policy = vi.policy\n",
    "expected_values = vi.V\n",
    "print optimal_policy\n",
    "print expected_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Putting it all together\n",
    "\n",
    "Here is the final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mdptoolbox.example\n",
    "import numpy as np\n",
    "prob = np.zeros((2, 10, 10)) \n",
    "#if leave\n",
    "prob[0] = [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "#if roll\n",
    "p=1.0/6\n",
    "prob[1] = [[0, p, p, p, 0, 0, 0, 0, 0, 0.5],\n",
    "           [0, 0, 0, 0, p, p, p, 0, 0, 0.5],\n",
    "           [0, 0, 0, 0, 0, p, p, p, 0, 0.5],\n",
    "           [0, 0, 0, 0, 0, 0, p, p, p, 0.5],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
    "np.sum(prob[0],axis=1)\n",
    "np.sum(prob[1],axis=1)\n",
    "\n",
    "rewards = np.zeros((2, 10, 10))\n",
    "# if leave\n",
    "rewards[0] = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "#if roll\n",
    "rewards[1] = [[0, 4, 5, 6, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 4, 5, 6, 0, 0, -4],\n",
    "            [0, 0, 0, 0, 0, 4, 5, 6, 0, -5],\n",
    "            [0, 0, 0, 0, 0, 0, 4, 5, 6, -6],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, -8],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, -9],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, -10],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, -11],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, -12],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "vi = mdptoolbox.mdp.ValueIteration(prob, rewards, 1)\n",
    "vi.run()\n",
    "\n",
    "optimal_policy = vi.policy\n",
    "expected_values = vi.V\n",
    "\n",
    "print optimal_policy\n",
    "print expected_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
